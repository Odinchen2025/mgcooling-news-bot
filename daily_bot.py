import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timezone, timedelta
import os

def load_keywords():
    """
    è®€å– keywords.txtï¼Œå¦‚æœæª”æ¡ˆä¸å­˜åœ¨å‰‡ä½¿ç”¨é è¨­å€¼
    """
    default_keywords = ["MGCooling", "AI Liquid Cooling", "AI æ°´å†·"]
    
    if not os.path.exists("keywords.txt"):
        print("âš ï¸ æ‰¾ä¸åˆ° keywords.txtï¼Œå°‡ä½¿ç”¨é è¨­é—œéµå­—ã€‚")
        return default_keywords

    with open("keywords.txt", "r", encoding="utf-8") as f:
        # è®€å–æ¯ä¸€è¡Œï¼Œå»é™¤ç©ºç™½ï¼Œä¸¦éæ¿¾æ‰ç©ºè¡Œ
        keywords = [line.strip() for line in f if line.strip()]
    
    print(f"âœ… å·²è¼‰å…¥ {len(keywords)} å€‹é—œéµå­—")
    return keywords

def fetch_google_news_rss(keyword):
    """
    é€é Google News RSS å–å¾—ç‰¹å®šé—œéµå­—çš„æ–°è
    """
    base_url = "https://news.google.com/rss/search"
    # è¨­å®šåƒæ•¸ï¼šq=é—œéµå­—, hl=èªè¨€(ç¹é«”ä¸­æ–‡), gl=åœ°å€(å°ç£), ceid=åœ°å€è¨­å®š
    params = {
        "q": keyword,
        "hl": "zh-TW",
        "gl": "TW",
        "ceid": "TW:zh-Hant"
    }
    
    try:
        response = requests.get(base_url, params=params, timeout=10)
        response.raise_for_status()
        return response.content
    except Exception as e:
        print(f"âŒ Error fetching news for {keyword}: {e}")
        return None

def parse_news(xml_content):
    """
    è§£æ XML æ ¼å¼çš„æ–°èè³‡æ–™
    """
    if not xml_content:
        return []
        
    news_items = []
    try:
        root = ET.fromstring(xml_content)
        # é€™è£¡æŠ“ 5 ç¯‡ä½œç‚ºç·©è¡ï¼Œä½†åœ¨ç”Ÿæˆå ±å‘Šæ™‚æˆ‘å€‘åªæœƒé¡¯ç¤ºå‰ 3 ç¯‡
        for item in root.findall('./channel/item')[:5]: 
            title = item.find('title').text
            link = item.find('link').text
            pub_date = item.find('pubDate').text
            
            # --- ğŸ› ï¸ æ¨™é¡Œæ¸…æ´—å€ ---
            # 1. å»é™¤æ¨™é¡Œå¾Œé¢çš„åª’é«”åç¨± (ä¾‹å¦‚ " - æ•¸ä½æ™‚ä»£")
            clean_title = title.split(' - ')[0]
            # 2. ã€é—œéµä¿®æ­£ã€‘æŠŠåŠå½¢ '|' æ›æˆå…¨å½¢ 'ï½œ'ï¼Œé¿å… GitHub æŠŠæ¨™é¡Œèª¤åˆ¤æˆè¡¨æ ¼
            clean_title = clean_title.replace('|', 'ï½œ')
            # 3. å»é™¤å¯èƒ½å°è‡´æ›è¡Œçš„ç¬¦è™Ÿ
            clean_title = clean_title.replace('\n', ' ')
            
            # å˜—è©¦ç°¡åŒ–æ—¥æœŸæ ¼å¼ (ä¾‹å¦‚å¾ "Wed, 04 Dec 2025 07:00:00 GMT" è®Šæ›´çŸ­ä¸€é»)
            # é€™è£¡ç›´æ¥ç”¨åŸå§‹å­—ä¸²ï¼Œä½†åœ¨é¡¯ç¤ºæ™‚æœƒç¸®å°å­—é«”
            
            news_items.append({
                'title': clean_title,
                'link': link,
                'pub_date': pub_date
            })
    except Exception as e:
        print(f"âš ï¸ XML Parsing Error: {e}")
        
    return news_items

def generate_markdown_report(all_news):
    """
    å°‡æ‰€æœ‰æ–°èå½™æ•´æˆ Markdown æ ¼å¼çš„å ±å‘Šï¼ŒåŒ…å«é‡é»æ‘˜è¦èˆ‡æŒ‰éˆ•
    """
    # è¨­å®šå°ç£æ™‚é–“ (UTC+8)
    tw_tz = timezone(timedelta(hours=8))
    today = datetime.now(tw_tz).strftime("%Y-%m-%d")
    
    content = f"# ğŸ§Š MGCooling AI æ°´å†·æ¯æ—¥æƒ…å ± - {today}\n\n"
    
    # --- ğŸ”˜ æ‰‹å‹•æ›´æ–°æŒ‰éˆ• (å…¬é–‹ç¶²é æœƒéš±è—ï¼Œå¾Œå°å¯è¦‹) ---
    repo_actions_url = "https://github.com/odinchen2025/mgcooling-news-bot/actions/workflows/daily_scan.yml"
    content += f"[![æ‰‹å‹•æ›´æ–°](https://img.shields.io/badge/æŒ‰æ­¤æ‰‹å‹•æ›´æ–°-Run_Update-2ea44f?style=for-the-badge&logo=github)]({repo_actions_url})\n\n"
    
    # --- ğŸ”¥ ç”Ÿæˆé‡é»æ‘˜è¦ (è‡ªå®¶æ–°èå„ªå…ˆæ’åº) ---
    content += "## ğŸ”¥ æœ¬æ—¥ç„¦é» (Top Highlights)\n"
    content += "> å¿«é€Ÿç€è¦½ç”¢æ¥­é ­æ¢ ï¼š\n\n"
    
    priority_highlights = [] # ç”¨ä¾†å­˜ MGCooling ç›¸é—œæ–°è
    general_highlights = []  # ç”¨ä¾†å­˜å…¶ä»–æ–°è
    processed_keys = set()   # è¨˜éŒ„å·²ç¶“è¢«é¸ä¸­çš„é—œéµå­—ï¼Œé¿å…é‡è¤‡
    
    # 1. ç¬¬ä¸€è¼ªç¯©é¸ï¼šå„ªå…ˆå°‹æ‰¾ MGCooling / å…ƒéˆ¦ ç›¸é—œæ–°è
    for keyword, items in all_news.items():
        if items:
            # å¦‚æœé—œéµå­—åŒ…å« "MGCooling" æˆ– "å…ƒéˆ¦"ï¼Œæ”¾å…¥å„ªå…ˆæ¸…å–®
            if "MGCooling" in keyword or "å…ƒéˆ¦" in keyword:
                top_item = items[0]
                # æ ¼å¼ï¼š1. [é—œéµå­—] æ¨™é¡Œ (æ—¥æœŸ)
                priority_highlights.append(f"1. **[{keyword}]** [{top_item['title']}]({top_item['link']}) <small>({top_item['pub_date']})</small>")
                processed_keys.add(keyword) # æ¨™è¨˜å·²è™•ç†

    # 2. ç¬¬äºŒè¼ªç¯©é¸ï¼šè£œå……å…¶ä»–ç”¢æ¥­æ–°è (æœ€å¤šè£œåˆ° 5-8 å‰‡)
    max_general_count = 5
    current_general_count = 0
    
    for keyword, items in all_news.items():
        if keyword in processed_keys:
            continue
            
        if items and current_general_count < max_general_count:
            top_item = items[0]
            general_highlights.append(f"1. **[{keyword}]** [{top_item['title']}]({top_item['link']}) <small>({top_item['pub_date']})</small>")
            current_general_count += 1
            
    # 3. åˆä½µæ¸…å–®
    final_highlights = priority_highlights + general_highlights
    
    if final_highlights:
        for line in final_highlights:
            content += line + "\n"
    else:
        content += "* ä»Šæ—¥ç„¡é‡å¤§æ–°èæ›´æ–°ã€‚\n"
    
    content += "\n---\n\n"
    
    # --- ğŸ“‹ ç”Ÿæˆè©³ç´°æ¸…å–® (åªé¡¯ç¤ºå‰ 3 å‰‡) ---
    content += "## ğŸ“‹ è©³ç´°æ–°èåˆ—è¡¨ (Latest 3)\n"
    
    if not all_news:
        content += "âš ï¸ ä»Šæ—¥æœå°‹ç„¡é‡å¤§æ›´æ–°ï¼Œæˆ–é€£ç·šç™¼ç”Ÿç•°å¸¸ã€‚\n"
    
    for keyword, items in all_news.items():
        content += f"### ğŸ” {keyword}\n"
        if not items:
            content += "* å°šç„¡æœ€æ–°ç›¸é—œæ–°èã€‚\n"
        
        # ã€ä¿®æ”¹é»ã€‘åªé¡¯ç¤ºå‰ 3 å‰‡ (items[:3])
        for item in items[:3]:
            content += f"- [{item['title']}]({item['link']})\n"
            # ã€ä¿®æ”¹é»ã€‘å°‡æ—¥æœŸé¡¯ç¤ºå‡ºä¾†ï¼Œä½¿ç”¨ç°è‰²å°å­—
            content += f"  - <small style='color:gray;'>ğŸ“… {item['pub_date']}</small>\n"
        content += "\n"
        
    content += "---\n"
    content += f"*Report generated at {datetime.now(tw_tz).strftime('%Y-%m-%d %H:%M:%S (Taipei Time)')}*\n"
    
    return content

def main():
    print("ğŸš€ é–‹å§‹åŸ·è¡Œæ¯æ—¥æ–°èæœé›†...")
    
    # 1. è®€å–é—œéµå­—
    keywords = load_keywords()
    
    all_news_data = {}
    
    # 2. çˆ¬å–æ–°è
    for kw in keywords:
        print(f"ğŸ“¡ æ­£åœ¨æœå°‹: {kw} ...")
        xml_data = fetch_google_news_rss(kw)
        items = parse_news(xml_data)
        all_news_data[kw] = items
        
    # 3. ç”Ÿæˆå ±å‘Š
    print("ğŸ“ æ­£åœ¨æ’°å¯«å ±å‘Š...")
    report_content = generate_markdown_report(all_news_data)
    
    # 4. å°‡çµæœå¯«å…¥ README.md
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(report_content)
        
    print("âœ… å ±å‘Šå·²æ›´æ–°è‡³ README.md")

if __name__ == "__main__":
    main()
