import requests
import xml.etree.ElementTree as ET
from datetime import datetime
import os

# è¨­å®šè¦æœå°‹çš„é—œéµå­— (ä½ å¯ä»¥éš¨æ™‚åœ¨é€™è£¡æ–°å¢)
# ä½¿ç”¨ URL ç·¨ç¢¼æˆ–æ˜¯ç›´æ¥ç”¨ä¸­æ–‡ï¼ŒGoogle News RSS é€šå¸¸æ”¯æ´
KEYWORDS = [
    "AI Liquid Cooling",
    "AI æ°´å†·",
    "MGCooling",
    "Immersion Cooling", # æµ¸æ²’å¼å†·å»
    "Direct-to-Chip Cooling" # ç›´æ¥æ™¶ç‰‡å†·å»
]

def fetch_google_news_rss(keyword):
    """
    é€é Google News RSS å–å¾—ç‰¹å®šé—œéµå­—çš„æ–°è
    """
    base_url = "https://news.google.com/rss/search"
    # è¨­å®šåƒæ•¸ï¼šq=é—œéµå­—, hl=èªè¨€(ç¹é«”ä¸­æ–‡), gl=åœ°å€(å°ç£), ceid=åœ°å€è¨­å®š
    params = {
        "q": keyword,
        "hl": "zh-TW",
        "gl": "TW",
        "ceid": "TW:zh-Hant"
    }
    
    try:
        response = requests.get(base_url, params=params, timeout=10)
        response.raise_for_status()
        return response.content
    except Exception as e:
        print(f"Error fetching news for {keyword}: {e}")
        return None

def parse_news(xml_content):
    """
    è§£æ XML æ ¼å¼çš„æ–°èè³‡æ–™
    """
    if not xml_content:
        return []
        
    news_items = []
    try:
        root = ET.fromstring(xml_content)
        # RSS çš„çµæ§‹é€šå¸¸æ˜¯ channel -> item
        for item in root.findall('./channel/item')[:5]: # æ¯å€‹é—œéµå­—åªæŠ“å‰ 5 ç¯‡æœ€æ–°çš„
            title = item.find('title').text
            link = item.find('link').text
            pub_date = item.find('pubDate').text
            
            news_items.append({
                'title': title,
                'link': link,
                'pub_date': pub_date
            })
    except Exception as e:
        print(f"XML Parsing Error: {e}")
        
    return news_items

def generate_markdown_report(all_news):
    """
    å°‡æ‰€æœ‰æ–°èå½™æ•´æˆ Markdown æ ¼å¼çš„å ±å‘Š
    """
    today = datetime.now().strftime("%Y-%m-%d")
    
    content = f"# ğŸ§Š MGCooling AI æ°´å†·æ¯æ—¥æƒ…å ± - {today}\n\n"
    content += "æœ¬å ±å‘Šç”± GitHub Actions è‡ªå‹•ç”Ÿæˆï¼Œå½™æ•´ç¶²è·¯ä¸Šæœ€æ–°çš„ç”¢æ¥­å‹•æ…‹ã€‚\n\n"
    
    if not all_news:
        content += "âš ï¸ ä»Šæ—¥æœå°‹ç„¡é‡å¤§æ›´æ–°ï¼Œæˆ–é€£ç·šç™¼ç”Ÿç•°å¸¸ã€‚\n"
    
    for keyword, items in all_news.items():
        content += f"## ğŸ” é—œéµå­—ï¼š{keyword}\n"
        if not items:
            content += "* å°šç„¡æœ€æ–°ç›¸é—œæ–°èã€‚\n"
        for item in items:
            # ç°¡å–®æ¸…ç†æ¨™é¡Œä¸­çš„ç¶²ç«™åç¨± (é€šå¸¸æ ¼å¼ç‚º Title - Source)
            clean_title = item['title'].split(' - ')[0]
            content += f"- **[{clean_title}]({item['link']})**\n"
            content += f"  - <small>ç™¼å¸ƒæ™‚é–“: {item['pub_date']}</small>\n"
        content += "\n"
        
    content += "---\n"
    content += f"*Report generated at {datetime.now().strftime('%H:%M:%S UTC')}*\n"
    
    return content

def main():
    print("é–‹å§‹åŸ·è¡Œæ¯æ—¥æ–°èæœé›†...")
    all_news_data = {}
    
    for kw in KEYWORDS:
        print(f"æ­£åœ¨æœå°‹: {kw} ...")
        xml_data = fetch_google_news_rss(kw)
        items = parse_news(xml_data)
        all_news_data[kw] = items
        
    report_content = generate_markdown_report(all_news_data)
    
    # å°‡çµæœå¯«å…¥ README.md (é€™æ¨£ä½ æ‰“é–‹ GitHub é¦–é å°±èƒ½çœ‹åˆ°)
    # ä¹Ÿå¯ä»¥æ”¹ç‚ºå¯«å…¥ daily_reports/date.md
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(report_content)
        
    print("å ±å‘Šå·²æ›´æ–°è‡³ README.md")

if __name__ == "__main__":
    main()
