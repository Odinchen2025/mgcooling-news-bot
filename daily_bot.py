import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timezone, timedelta
import os
import json
import re
from email.utils import parsedate_to_datetime

def load_keywords():
    """ è®€å– keywords.txt """
    default_keywords = ["MGCooling", "AI Liquid Cooling", "AI æ°´å†·"]
    if not os.path.exists("keywords.txt"):
        print("âš ï¸ æ‰¾ä¸åˆ° keywords.txtï¼Œå°‡ä½¿ç”¨é è¨­é—œéµå­—ã€‚")
        return default_keywords
    with open("keywords.txt", "r", encoding="utf-8") as f:
        keywords = [line.strip() for line in f if line.strip()]
    print(f"âœ… å·²è¼‰å…¥ {len(keywords)} å€‹é—œéµå­—")
    return keywords

def fetch_google_news_rss(keyword):
    """ é€é Google News RSS å–å¾—ç‰¹å®šé—œéµå­—çš„æ–°è """
    base_url = "https://news.google.com/rss/search"
    params = {
        "q": keyword,
        "hl": "zh-TW",
        "gl": "TW",
        "ceid": "TW:zh-Hant"
    }
    try:
        response = requests.get(base_url, params=params, timeout=10)
        response.raise_for_status()
        return response.content
    except Exception as e:
        print(f"âŒ Error fetching news for {keyword}: {e}")
        return None

def clean_html_tags(text):
    """ æ¸…é™¤æ‘˜è¦ä¸­çš„ HTML æ¨™ç±¤ """
    if not text: return ""
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

def parse_news(xml_content):
    """ è§£æ XML æ ¼å¼çš„æ–°èè³‡æ–™ """
    if not xml_content:
        return []
    news_items = []
    try:
        root = ET.fromstring(xml_content)
        # æŠ“å–å‰ 5 ç¯‡ä½œç‚ºç·©è¡
        for item in root.findall('./channel/item')[:5]: 
            title = item.find('title').text
            link = item.find('link').text
            pub_date_raw = item.find('pubDate').text
            
            # --- ğŸ› ï¸ æ—¥æœŸæ ¼å¼åŒ– (M/D/YY) ---
            try:
                # ä½¿ç”¨ email.utils è§£æ RFC 822 æ ¼å¼
                dt_obj = parsedate_to_datetime(pub_date_raw)
                # è½‰æˆ M/D/YY å­—ä¸² (ä¾‹å¦‚ 4/26/25)
                pub_date = f"{dt_obj.month}/{dt_obj.day}/{dt_obj.strftime('%y')}"
            except Exception as e:
                # å¦‚æœè§£æå¤±æ•—ï¼Œç¶­æŒåŸæ¨£
                pub_date = pub_date_raw
            
            description_node = item.find('description')
            raw_desc = description_node.text if description_node is not None else ""
            clean_desc = clean_html_tags(raw_desc)
            
            clean_title = title.split(' - ')[0]
            clean_title = clean_title.replace('|', 'ï½œ').replace('\n', ' ')
            
            news_items.append({
                'title': clean_title,
                'link': link,
                'pub_date': pub_date,
                'description': clean_desc
            })
    except Exception as e:
        print(f"âš ï¸ XML Parsing Error: {e}")
    return news_items

def generate_markdown_report(all_news):
    """ å°‡æ‰€æœ‰æ–°èå½™æ•´æˆ Markdown æ ¼å¼çš„å ±å‘Š """
    tw_tz = timezone(timedelta(hours=8))
    
    # å–å¾—ç¾åœ¨æ™‚é–“
    now = datetime.now(tw_tz)
    # æ¨™é¡Œæ—¥æœŸæ ¼å¼ M/D/YY
    today = f"{now.month}/{now.day}/{now.strftime('%y')}"
    # è©³ç´°æ›´æ–°æ™‚é–“ YYYY/MM/DD HH:MM
    update_time_str = now.strftime('%Y/%m/%d %H:%M')
    
    content = f"# ğŸ§Š MGCooling AI æ°´å†·æ¯æ—¥æƒ…å ± - {today}\n\n"
    
    repo_actions_url = "https://github.com/odinchen2025/mgcooling-news-bot/actions/workflows/daily_scan.yml"
    
    # æ›´æ–°æŒ‰éˆ•
    content += f"[![æ‰‹å‹•æ›´æ–°](https://img.shields.io/badge/æŒ‰æ­¤æ‰‹å‹•æ›´æ–°-Run_Update-2ea44f?style=for-the-badge&logo=github)]({repo_actions_url})\n"
    
    # --- ğŸ•’ æ–°å¢ï¼šæ·¡æ·¡ç°ç™½è‰²æ›´æ–°æ™‚é–“ (é å³å°é½Š) ---
    content += f"<p align='right' style='color: #bfbfbf; font-size: 13px; margin-top: -20px;'>æ›´æ–°æ™‚é–“ï¼š{update_time_str}</p>\n\n"
    
    # --- ğŸ”¥ ç”Ÿæˆé‡é»æ‘˜è¦ ---
    content += "## ğŸ”¥ æœ¬æ—¥ç„¦é» (Top Highlights)\n"
    content += "> å¿«é€Ÿç€è¦½ç”¢æ¥­é ­æ¢ï¼š\n\n"
    
    priority_highlights = []
    general_highlights = []
    processed_keys = set()
    
    # å„ªå…ˆç¯©é¸
    for keyword, items in all_news.items():
        if items:
            if "MGCooling" in keyword or "å…ƒéˆ¦" in keyword:
                top_item = items[0]
                priority_highlights.append(f"1. **[{keyword}]** [{top_item['title']}]({top_item['link']}) <small>({top_item['pub_date']})</small>")
                processed_keys.add(keyword)

    # ä¸€èˆ¬ç¯©é¸
    max_general_count = 5
    current_general_count = 0
    for keyword, items in all_news.items():
        if keyword in processed_keys: continue
        if items and current_general_count < max_general_count:
            top_item = items[0]
            general_highlights.append(f"1. **[{keyword}]** [{top_item['title']}]({top_item['link']}) <small>({top_item['pub_date']})</small>")
            current_general_count += 1
            
    final_highlights = priority_highlights + general_highlights
    if final_highlights:
        for line in final_highlights: content += line + "\n"
    else:
        content += "* ä»Šæ—¥ç„¡é‡å¤§æ–°èæ›´æ–°ã€‚\n"
    
    content += "\n---\n\n"
    
    # --- ğŸ“‹ ç”Ÿæˆè©³ç´°æ¸…å–® (ä¾æ“šæœ€æ–°åˆ°æœ€èˆŠ) ---
    content += "## ğŸ“‹ è©³ç´°æ–°èåˆ—è¡¨\n"
    
    for keyword, items in all_news.items():
        content += f"### ğŸ” {keyword}\n"
        if not items:
            content += "* å°šç„¡æœ€æ–°ç›¸é—œæ–°èã€‚\n"
        
        # åªé¡¯ç¤ºå‰ 3 å‰‡
        for item in items[:3]:
            # æ—¥æœŸåœ¨æ¨™é¡Œæœ€å‰é¢ï¼Œæ·ºç°è‰²ï¼Œæ ¼å¼ M/D/YY
            content += f"- <small style='color:gray;'>{item['pub_date']}</small> [{item['title']}]({item['link']})\n"
        content += "\n"
    
    content += "---\n"
    content += f"*Report generated at {datetime.now(tw_tz).strftime('%Y-%m-%d %H:%M:%S (Taipei Time)')}*\n"
    return content

def main():
    print("ğŸš€ é–‹å§‹åŸ·è¡Œæ¯æ—¥æ–°èæœé›†...")
    keywords = load_keywords()
    all_news_data = {}
    
    # 1. çˆ¬å–æ–°è
    for kw in keywords:
        print(f"ğŸ“¡ æ­£åœ¨æœå°‹: {kw} ...")
        xml_data = fetch_google_news_rss(kw)
        items = parse_news(xml_data)
        all_news_data[kw] = items
        
    # 2. ç”Ÿæˆ Markdown å ±å‘Š
    print("ğŸ“ æ­£åœ¨æ’°å¯« Markdown å ±å‘Š...")
    report_content = generate_markdown_report(all_news_data)
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(report_content)
        
    # 3. è¼¸å‡º JSON è³‡æ–™
    print("ğŸ’¾ æ­£åœ¨è¼¸å‡º JSON è³‡æ–™...")
    with open("news.json", "w", encoding="utf-8") as f:
        json.dump(all_news_data, f, ensure_ascii=False, indent=4)

    print("âœ… å ±å‘Šèˆ‡è³‡æ–™å·²æ›´æ–° (README.md & news.json)")

if __name__ == "__main__":
    main()
