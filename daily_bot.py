import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timezone, timedelta
import os

def load_keywords():
    """
    è®€å– keywords.txtï¼Œå¦‚æœæª”æ¡ˆä¸å­˜åœ¨å‰‡ä½¿ç”¨é è¨­å€¼
    """
    default_keywords = ["AI Liquid Cooling", "AI æ°´å†·", "MGCooling"]
    
    if not os.path.exists("keywords.txt"):
        print("âš ï¸ æ‰¾ä¸åˆ° keywords.txtï¼Œå°‡ä½¿ç”¨é è¨­é—œéµå­—ã€‚")
        return default_keywords

    with open("keywords.txt", "r", encoding="utf-8") as f:
        # è®€å–æ¯ä¸€è¡Œï¼Œå»é™¤ç©ºç™½ï¼Œä¸¦éæ¿¾æ‰ç©ºè¡Œ
        keywords = [line.strip() for line in f if line.strip()]
    
    print(f"âœ… å·²è¼‰å…¥ {len(keywords)} å€‹é—œéµå­—")
    return keywords

def fetch_google_news_rss(keyword):
    """
    é€é Google News RSS å–å¾—ç‰¹å®šé—œéµå­—çš„æ–°è
    """
    base_url = "https://news.google.com/rss/search"
    # è¨­å®šåƒæ•¸ï¼šq=é—œéµå­—, hl=èªè¨€(ç¹é«”ä¸­æ–‡), gl=åœ°å€(å°ç£), ceid=åœ°å€è¨­å®š
    params = {
        "q": keyword,
        "hl": "zh-TW",
        "gl": "TW",
        "ceid": "TW:zh-Hant"
    }
    
    try:
        response = requests.get(base_url, params=params, timeout=10)
        response.raise_for_status()
        return response.content
    except Exception as e:
        print(f"âŒ Error fetching news for {keyword}: {e}")
        return None

def parse_news(xml_content):
    """
    è§£æ XML æ ¼å¼çš„æ–°èè³‡æ–™
    """
    if not xml_content:
        return []
        
    news_items = []
    try:
        root = ET.fromstring(xml_content)
        # æ¯å€‹é—œéµå­—åªæŠ“å‰ 5 ç¯‡
        for item in root.findall('./channel/item')[:5]: 
            title = item.find('title').text
            link = item.find('link').text
            pub_date = item.find('pubDate').text
            
            # --- ğŸ› ï¸ æ¨™é¡Œæ¸…æ´—å€ ---
            # 1. å»é™¤æ¨™é¡Œå¾Œé¢çš„åª’é«”åç¨± (ä¾‹å¦‚ " - æ•¸ä½æ™‚ä»£")
            clean_title = title.split(' - ')[0]
            # 2. ã€é—œéµä¿®æ­£ã€‘æŠŠåŠå½¢ '|' æ›æˆå…¨å½¢ 'ï½œ'ï¼Œé¿å… GitHub æŠŠæ¨™é¡Œèª¤åˆ¤æˆè¡¨æ ¼
            clean_title = clean_title.replace('|', 'ï½œ')
            # 3. å»é™¤å¯èƒ½å°è‡´æ›è¡Œçš„ç¬¦è™Ÿ
            clean_title = clean_title.replace('\n', ' ')
            
            news_items.append({
                'title': clean_title,
                'link': link,
                'pub_date': pub_date
            })
    except Exception as e:
        print(f"âš ï¸ XML Parsing Error: {e}")
        
    return news_items

def generate_markdown_report(all_news):
    """
    å°‡æ‰€æœ‰æ–°èå½™æ•´æˆ Markdown æ ¼å¼çš„å ±å‘Šï¼ŒåŒ…å«é‡é»æ‘˜è¦
    """
    # è¨­å®šå°ç£æ™‚é–“ (UTC+8)
    tw_tz = timezone(timedelta(hours=8))
    today = datetime.now(tw_tz).strftime("%Y-%m-%d")
    
    content = f"# ğŸ§Š MGCooling AI æ°´å†·æ¯æ—¥æƒ…å ± - {today}\n\n"
    
    # --- ğŸ”˜ æ–°å¢ï¼šæ‰‹å‹•æ›´æ–°æŒ‰éˆ• ---
    # é€™å€‹é€£çµæœƒå¸¶ä½¿ç”¨è€…åˆ° GitHub Actions çš„åŸ·è¡Œé é¢
    # ç‚ºäº†æ–¹ä¾¿ï¼Œé€™è£¡ç›´æ¥å¡«å…¥ä½ çš„å°ˆæ¡ˆè·¯å¾‘
    repo_actions_url = "https://github.com/odinchen2025/mgcooling-news-bot/actions/workflows/daily_scan.yml"
    content += f"[![æ‰‹å‹•æ›´æ–°](https://img.shields.io/badge/æŒ‰æ­¤æ‰‹å‹•æ›´æ–°-Run%20Update-2ea44f?style=for-the-badge&logo=github)]({repo_actions_url})\n\n"
    
    content += "æœ¬å ±å‘Šç”± GitHub Actions è‡ªå‹•ç”Ÿæˆï¼Œå½™æ•´ç¶²è·¯ä¸Šæœ€æ–°çš„ç”¢æ¥­å‹•æ…‹ã€‚\n\n"
    
    # --- ğŸ”¥ é‡é»æ‘˜è¦å€å¡Š (Top Highlights) ---
    content += "## ğŸ”¥ æœ¬æ—¥ç„¦é» (Top Highlights)\n"
    content += "> å¿«é€Ÿç€è¦½å„é—œéµå­—çš„é ­æ¢æ–°èï¼š\n\n"
    
    has_highlights = False
    highlight_count = 0
    
    # é‚è¼¯ï¼šå¾æ¯å€‹é—œéµå­—é¡åˆ¥ä¸­ï¼ŒæŒ‘é¸ã€Œç¬¬ä¸€å‰‡ã€æ–°èä½œç‚ºé‡é»ï¼Œæœ€å¤šæŒ‘ 5 å‰‡
    for keyword, items in all_news.items():
        if items and highlight_count < 5:
            top_item = items[0] # å–è©²é¡åˆ¥çš„ç¬¬ä¸€ç¯‡
            # æ ¼å¼ï¼š1. [é—œéµå­—] æ–°èæ¨™é¡Œ
            content += f"1. **[{keyword}]** [{top_item['title']}]({top_item['link']})\n"
            highlight_count += 1
            has_highlights = True
            
    if not has_highlights:
        content += "* ä»Šæ—¥ç„¡é‡å¤§æ–°èæ›´æ–°ã€‚\n"
    
    content += "\n---\n\n"
    
    # --- ğŸ“‹ è©³ç´°æ¸…å–®å€å¡Š ---
    content += "## ğŸ“‹ è©³ç´°æ–°èåˆ—è¡¨\n"
    
    if not all_news:
        content += "âš ï¸ ä»Šæ—¥æœå°‹ç„¡é‡å¤§æ›´æ–°ï¼Œæˆ–é€£ç·šç™¼ç”Ÿç•°å¸¸ã€‚\n"
    
    for keyword, items in all_news.items():
        content += f"### ğŸ” {keyword}\n"
        if not items:
            content += "* å°šç„¡æœ€æ–°ç›¸é—œæ–°èã€‚\n"
        for item in items:
            content += f"- [{item['title']}]({item['link']})\n"
        content += "\n"
        
    content += "---\n"
    content += f"*Report generated at {datetime.now(tw_tz).strftime('%Y-%m-%d %H:%M:%S (Taipei Time)')}*\n"
    
    return content

def main():
    print("ğŸš€ é–‹å§‹åŸ·è¡Œæ¯æ—¥æ–°èæœé›†...")
    
    # 1. è®€å–é—œéµå­—
    keywords = load_keywords()
    
    all_news_data = {}
    
    # 2. çˆ¬å–æ–°è
    for kw in keywords:
        print(f"ğŸ“¡ æ­£åœ¨æœå°‹: {kw} ...")
        xml_data = fetch_google_news_rss(kw)
        items = parse_news(xml_data)
        all_news_data[kw] = items
        
    # 3. ç”Ÿæˆå ±å‘Š
    print("ğŸ“ æ­£åœ¨æ’°å¯«å ±å‘Š...")
    report_content = generate_markdown_report(all_news_data)
    
    # 4. å°‡çµæœå¯«å…¥ README.md
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(report_content)
        
    print("âœ… å ±å‘Šå·²æ›´æ–°è‡³ README.md")

if __name__ == "__main__":
    main()
